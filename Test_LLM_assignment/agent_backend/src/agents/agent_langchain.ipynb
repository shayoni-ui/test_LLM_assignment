{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27c4fbad0a904765b82b87838210bcec\n",
            "/mnt/batch/tasks/shared/LS_root/mounts/clusters/shayonisimms/code\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "# Add parent directory to sys.path\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(\"/home/azureuser/cloudfiles/code/Users/shayoni.x.dutta/Test_LLM_FinCONV/agent_backend/azureml/agents/agent_langchain.ipynb\"), \"..\")))\n",
        "# Now import agent_config\n",
        "from agent_config import *\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "234b0e75-3299-4ae7-931d-05ab9bae66b6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "gather": {
          "logged": 1747521800388
        }
      },
      "outputs": [],
      "source": [
        "cmd = f'pip install -r /home/azureuser/cloudfiles/code/Users/shayoni.x.dutta/Test_LLM_FinCONV/agent_backend/azureml/requirements.txt --quiet'\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain langchain-openai pandasql --quiet\n",
        "%pip install duckdb --quiet\n",
        "#%pip install langchain-community --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "aac1125c-8744-4ecb-83ee-a05eb7a9c7a4",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "gather": {
          "logged": 1747521768978
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "from typing import Any, List, Optional, Type\n",
        "\n",
        "import mlflow  # type: ignore\n",
        "import pandas as pd\n",
        "# from databricks.sdk.runtime import *  # type: ignore # noqa\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain.agents.agent import RunnableAgent, RunnableMultiActionAgent\n",
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from pydantic import BaseModel\n",
        "from langchain.tools import BaseTool\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    PromptTemplate,\n",
        ")\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "# from mlflow.types.llm import (  # Non-streaming helper classes\n",
        "#     ChatChoice,\n",
        "#     ChatCompletionResponse,\n",
        "#     ChatMessage,\n",
        "#     ChatParams,\n",
        "# )\n",
        "#from pandasql import sqldf\n",
        "import duckdb\n",
        "\n",
        "\n",
        "from utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ca611509-ebdb-4428-859b-a3b9545e2eb7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "gather": {
          "logged": 1740428975780
        }
      },
      "outputs": [],
      "source": [
        "# Running the agent_config notebook to iniate the constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ad423e1a-5cf0-4ce3-bca8-68f1c9c5df06",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "gather": {
          "logged": 1740428984504
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azureml/mlflow/_protos/aml_service_pb2.py:10: UserWarning: google.protobuf.service module is deprecated. RPC implementations should provide code generator plugins which generate code specific to the RPC implementation. service.py will be removed in Jan 2025\n",
            "  from google.protobuf import service as _service\n",
            "2025/05/20 13:44:15 INFO mlflow.tracking.fluent: Experiment with name '/home/azureuser/cloudfiles/code/Users/shayoni.x.dutta/Test_LLM_FinCONV/agent_backend/azureml' does not exist. Creating a new experiment.\n"
          ]
        }
      ],
      "source": [
        "# MLFlow Setup\n",
        "\n",
        "experiment = mlflow.set_experiment(\"/home/azureuser/cloudfiles/code/Users/shayoni.x.dutta/Test_LLM_FinCONV/agent_backend/azureml\")  # type: ignore  # noqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "11b54c02-3c59-4449-b1e6-83133d75ff67",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "gather": {
          "logged": 1740428995403
        }
      },
      "outputs": [],
      "source": [
        "token_provider = get_bearer_token_provider_kong()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "249bc52b-578c-423d-8ab1-51c1408f595a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "gather": {
          "logged": 1740428997322
        }
      },
      "outputs": [],
      "source": [
        "files_metadata, csv_files_data = generate_source_data(f'/home/azureuser/cloudfiles/code/Users/shayoni.x.dutta/Test_LLM_FinCONV/agent_backend/datasources')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting up the agent's tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a60946d1-53a2-42c7-9dd3-7d7f9fe29a52",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "gather": {
          "logged": 1740429014142
        }
      },
      "outputs": [],
      "source": [
        "from pydantic import Field\n",
        "\n",
        "class GetDatasetMetadaInput(BaseModel):\n",
        "    \"\"\"\n",
        "    Abstract base class for creating input for agent tools.\n",
        "\n",
        "    Attributes:\n",
        "        <var> (<type>): Variable names that tool will use as input arguments.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    question: Optional[str] = Field(description=\"The prompt/request from the user\", default=None)\n",
        "\n",
        "\n",
        "class GetDatasetMetadaTool(BaseTool):\n",
        "    \"\"\"\n",
        "    Abstract base class for creating agent tools.\n",
        "\n",
        "    Attributes:\n",
        "        name (str): The name of the agent tool.\n",
        "        description (str): A brief description of the agent tool.\n",
        "        args_schema (Type[BaseModel]): The schema model for the arguments the agent tool accepts.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    name: str = \"get_file_metadata\"\n",
        "    description: str = (\n",
        "        \"Provides the metada, (like description, schema and sample) of the dataset (multiple CSV files) to be used for inference by the agent.\"\n",
        "    )\n",
        "    args_schema: Type[BaseModel] = GetDatasetMetadaInput\n",
        "\n",
        "    def _run(self, question: Optional[str] = None) -> Any:\n",
        "        \"\"\"\n",
        "        Synchronously run the agent tool to provide the appropriate file name and file schema to\n",
        "        answer the user's question.\n",
        "\n",
        "        Args:\n",
        "            question (str): The question asked by user in input terminal.\n",
        "\n",
        "        Returns:\n",
        "            It provides a dictionary with the metadata (file name, description, schema and sample).\n",
        "        \"\"\"\n",
        "\n",
        "        return json.dumps(files_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "730f4683-d5b5-491b-b988-6e242d203199",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "gather": {
          "logged": 1740429015308
        }
      },
      "outputs": [],
      "source": [
        "class GetTargetFileSchemaInput(BaseModel):\n",
        "    \"\"\"\n",
        "    Abstract base class for creating input for agent tools.\n",
        "\n",
        "    Attributes:\n",
        "        <var> (<type>): Variable names that tool will use as input arguments.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    question: str = Field(description=\"The prompt/request from the user\")\n",
        "    files_metadata: str = Field(\n",
        "        description=\"A dictionary containing the metadata about the CSV files in the dataset.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class GetTargetFileSchemaTool(BaseTool):\n",
        "    \"\"\"\n",
        "    Abstract base class for creating agent tools.\n",
        "\n",
        "    Attributes:\n",
        "        name (str): The name of the agent tool.\n",
        "        description (str): A brief description of the agent tool.\n",
        "        args_schema (Type[BaseModel]): The schema model for the arguments the agent tool accepts.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    name: str = \"get_target_file_schema\"\n",
        "    description: str = (\n",
        "        \"Provides the most likely file_name and file_schema to be used for querying based on \\\n",
        "         the input question and the files/dataset metadata.\"\n",
        "    )\n",
        "    args_schema: Type[BaseModel] = GetTargetFileSchemaInput\n",
        "\n",
        "    def _run(self, question: str, files_metadata: str) -> Any:\n",
        "        \"\"\"\n",
        "        Synchronously run the agent tool to provide the appropriate file name and file schema to\n",
        "        answer the user's question.\n",
        "\n",
        "        Args:\n",
        "            question (str): The question asked by user in input terminal.\n",
        "\n",
        "        Returns:\n",
        "            It provides with relevant file_name and file schema from where question belongs to.\n",
        "        \"\"\"\n",
        "\n",
        "        response_schemas = [\n",
        "            ResponseSchema(\n",
        "                name=\"file_name\",\n",
        "                description=\"The selected file's name.\",\n",
        "            ),\n",
        "            ResponseSchema(\n",
        "                name=\"file_schema\",\n",
        "                description=\"The select file's schema.\",\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "        format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "        format_instructions += (\n",
        "            ' make sure to format it properly to be used as JSON, like enclosing strings with \".'\n",
        "        )\n",
        "\n",
        "        prompt_text = \"\"\"\n",
        "        Given the files listed below and their respective file schema, select the most \\\n",
        "        relevant file to answer the following question: '{question}'.\n",
        "        {files_metadata}.\n",
        "        {format_instructions}.\n",
        "        You can remove the sample data from the file_schema, just pass a string with each column separated by commas.\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = PromptTemplate.from_template(\n",
        "            template=prompt_text,\n",
        "            partial_variables={\n",
        "                \"format_instructions\": format_instructions,\n",
        "                \"files_metadata\": files_metadata,\n",
        "            },\n",
        "        )\n",
        "        _input = {\"question\": question}\n",
        "\n",
        "        model = AzureChatOpenAI(\n",
        "            model=\"o1\",\n",
        "            azure_endpoint=\"https://dev.api.gsk.com/co/psc/azureopenai/us6/\", # os.getenv(\"o1_AZURE_OPENAI_ENDPOINT\"),\n",
        "            azure_deployment=\"o1\", # os.getenv(\"o1_AZURE_OPENAI_DEPLOYMENT\"),\n",
        "            api_version= \"2024-12-01-preview\", # os.getenv(\"o1_AZURE_OPENAI_API_VERSION\"),\n",
        "            azure_ad_token_provider=token_provider\n",
        "        )\n",
        "\n",
        "        chain = prompt | model | output_parser\n",
        "\n",
        "        res = chain.invoke(_input)\n",
        "\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4e6b90d0-4a60-4bbc-8153-32d37400f4c7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "gather": {
          "logged": 1740429015609
        }
      },
      "outputs": [],
      "source": [
        "class GenerateSQLForFileInput(BaseModel):\n",
        "    \"\"\"\n",
        "    Abstract base class for creating input for agent tools.\n",
        "\n",
        "    Attributes:\n",
        "        <var> (<type>): Variable names that tool will use as input arguments.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    question: str = Field(\n",
        "        description=\"The prompt/request from the user to generate the SQL query for.\"\n",
        "    )\n",
        "    file_name: str = Field(\n",
        "        description=\"The file name that represents the table name to be used in the SQL query\"\n",
        "    )\n",
        "    file_schema: str = Field(\n",
        "        description=\"A dictionary represented as string representing a sample of the file's \\\n",
        "                     schema to be used in the SQL query\"\n",
        "    )\n",
        "\n",
        "\n",
        "class GenerateSQLForFileTool(BaseTool):\n",
        "    \"\"\"\n",
        "    Abstract base class for creating agent tools.\n",
        "\n",
        "    Attributes:\n",
        "        name (str): The name of the agent tool.\n",
        "        description (str): A brief description of the agent tool.\n",
        "        args_schema (Type[BaseModel]): The schema model for the arguments the agent tool accepts.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    name: str = \"generate_sql_for_file\"\n",
        "    description: str = (\n",
        "        \"\"\"Provides the SQL query to used to answer the user's request based on a \\\n",
        "        file_name (that represent the table name) and a file_schema \\\n",
        "        (that represents the colums in the table)\"\"\"\n",
        "    )\n",
        "    args_schema: Type[BaseModel] = GenerateSQLForFileInput\n",
        "\n",
        "    def _run(self, question: str, file_name: str, file_schema: str) -> Any:\n",
        "        \"\"\"\n",
        "        Synchronously run the agent tool to provide sql query to be ran on the datasource.\n",
        "\n",
        "        Args:\n",
        "            question (str): The question asked by user in input terminal.\n",
        "            file_name (str): Relevant file name generated by GetTargetFileSchemaTool.\n",
        "            file_schema (str): Relevant file schema generated by GetTargetFileSchemaTool.\n",
        "\n",
        "        Returns:\n",
        "            It provides with relevant SQL query with file_name as table and column name\n",
        "            refers in file_schema.\n",
        "        \"\"\"\n",
        "\n",
        "        response_schemas = [\n",
        "            ResponseSchema(\n",
        "                name=\"sql_query\",\n",
        "                description=\"SQL query that will be returned\",\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "        format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "        format_instructions += \" make sure to format it properly to be used as plain string \\\n",
        "                                 like one sentence with SQL query.\"\n",
        "\n",
        "        prompt_text = \"\"\"\n",
        "        Given the following file name '{file_name}', that is supposed\n",
        "        to be used as the SQL table name\n",
        "        and the following JSON representing the schema of the columns in that\n",
        "        table:\n",
        "\n",
        "        {file_schema}\n",
        "\n",
        "        Please provide a SQL query that answers the following question: '{question}'.\n",
        "\n",
        "        When string values are to be used as filters, make sure\n",
        "        both the column and value have lower case to avoid case sensitive issues.\n",
        "\n",
        "        Just answer with the SQL query and nothing else.\n",
        "\n",
        "        {format_instructions}\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        model = AzureChatOpenAI(\n",
        "            model=\"o1\",\n",
        "            azure_endpoint=\"https://dev.api.gsk.com/co/psc/azureopenai/us6/\", # os.getenv(\"o1_AZURE_OPENAI_ENDPOINT\"),\n",
        "            azure_deployment=\"o1\", # os.getenv(\"o1_AZURE_OPENAI_DEPLOYMENT\"),\n",
        "            api_version= \"2024-12-01-preview\", # os.getenv(\"o1_AZURE_OPENAI_API_VERSION\"),\n",
        "            azure_ad_token_provider=token_provider\n",
        "        )\n",
        "\n",
        "        prompt = PromptTemplate.from_template(\n",
        "            template=prompt_text, partial_variables={\"format_instructions\": format_instructions}\n",
        "        )\n",
        "        _input = {\"question\": question, \"file_name\": file_name, \"file_schema\": file_schema}\n",
        "\n",
        "        chain = prompt | model | output_parser\n",
        "\n",
        "        res = chain.invoke(_input)[\"sql_query\"]\n",
        "\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2f28386f-e9cc-48ac-982b-ad4cfc4e79eb",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "gather": {
          "logged": 1740429016495
        }
      },
      "outputs": [],
      "source": [
        "class GenerateAnswerforQuestionInput(BaseModel):\n",
        "    \"\"\"\n",
        "\n",
        "    Abstract base class for creating input for agent tools.\n",
        "\n",
        "    Attributes:\n",
        "        <var> (<type>): Variable names that tool will use as input arguments.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    question: str = Field(description=\"The initial prompt/request from the user.\")\n",
        "    sql_query: str = Field(description=\"The SQL query to be executed on the database\")\n",
        "    file_name: str = Field(\n",
        "        description=\"The file name that represents the table name to be used in the SQL query\"\n",
        "    )\n",
        "    file_schema: str = Field(\n",
        "        description=\"A dictionary represented as string representing a sample of the file's \\\n",
        "                     schema to be used in the SQL query\"\n",
        "    )\n",
        "\n",
        "\n",
        "class GenerateAnswerforQuestionTool(BaseTool):\n",
        "    \"\"\"\n",
        "    Abstract base class for creating agent tools.\n",
        "\n",
        "    Attributes:\n",
        "        name (str): The name of the agent tool.\n",
        "        description (str): A brief description of the agent tool.\n",
        "        args_schema (Type[BaseModel]): The schema model for the arguments the agent tool accepts.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    name: str = \"generate_answer_for_question\"\n",
        "    description: str = \"\"\"Provides answer once the SQL query gets executed over database\"\"\"\n",
        "    args_schema: Type[BaseModel] = GenerateAnswerforQuestionInput\n",
        "\n",
        "    def _run(self, question: str, sql_query: str, file_name: str, file_schema: str) -> Any:\n",
        "        \"\"\"\n",
        "        Synchronously run the agent tool to provide the result from sql query..\n",
        "\n",
        "        Args:\n",
        "            question (str): The question asked by user in input terminal.\n",
        "            file_name (str): Relevant file name generated by GetTargetFileSchemaTool.\n",
        "            file_schema (str): Relevant file schema generated by GetTargetFileSchemaTool.\n",
        "            sql_query (str): Relevant sql query generated from  GenerateSQLForFileTool\n",
        "\n",
        "        Returns:\n",
        "            It execute sql query and provides result in dict format.\n",
        "        \"\"\"\n",
        "\n",
        "        parse_sql_query = sql_query.split(\" \")\n",
        "\n",
        "        parse_sql_query[parse_sql_query.index(\"FROM\") + 1] = \"dftbl\"\n",
        "\n",
        "        new_query = \" \".join(parse_sql_query)\n",
        "\n",
        "        dftbl = pd.DataFrame(csv_files_data[file_name])\n",
        "\n",
        "        #result_df = sqldf(new_query, locals())\n",
        "        result_df = duckdb.query(new_query).to_df()\n",
        "\n",
        "\n",
        "        return result_df.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Type, Any\n",
        "from pydantic import BaseModel, Field\n",
        "import pandas as pd\n",
        "import duckdb\n",
        "\n",
        "\n",
        "class GenerateAnswerforQuestionInput(BaseModel):\n",
        "    \"\"\"\n",
        "    Input schema for GenerateAnswerforQuestionTool.\n",
        "\n",
        "    Attributes:\n",
        "        question (str): The initial prompt/request from the user.\n",
        "        sql_query (str): The SQL query to be executed on the database.\n",
        "        file_name (str): The file name that represents the table name to be used in the SQL query.\n",
        "        file_schema (str): A dictionary represented as a string showing a sample schema of the file.\n",
        "    \"\"\"\n",
        "    question: str = Field(description=\"The initial prompt/request from the user.\")\n",
        "    sql_query: str = Field(description=\"The SQL query to be executed on the database.\")\n",
        "    file_name: str = Field(\n",
        "        description=\"The file name that represents the table name to be used in the SQL query.\"\n",
        "    )\n",
        "    file_schema: str = Field(\n",
        "        description=\"A dictionary represented as string representing a sample of the file's schema to be used in the SQL query.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class GenerateAnswerforQuestionTool(BaseTool):\n",
        "    \"\"\"\n",
        "    Tool to execute SQL query on a dataframe extracted from a file and return the result.\n",
        "\n",
        "    Attributes:\n",
        "        name (str): Name of the tool.\n",
        "        description (str): Description of what the tool does.\n",
        "        args_schema (Type[BaseModel]): Pydantic schema for expected inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    name: str = \"generate_answer_for_question\"\n",
        "    description: str = \"Provides answer once the SQL query gets executed over database\"\n",
        "    args_schema: Type[BaseModel] = GenerateAnswerforQuestionInput\n",
        "\n",
        "    def _run(self, question: str, sql_query: str, file_name: str, file_schema: str) -> Any:\n",
        "        \"\"\"\n",
        "        Executes a SQL query on the loaded dataframe and returns the result.\n",
        "\n",
        "        Args:\n",
        "            question (str): User's question.\n",
        "            sql_query (str): SQL query to be executed.\n",
        "            file_name (str): CSV file name to load as a dataframe.\n",
        "            file_schema (str): Schema of the file (not used in query execution).\n",
        "\n",
        "        Returns:\n",
        "            dict: The result of the query as a dictionary.\n",
        "        \"\"\"\n",
        "        # Split query and search for 'FROM' (case-insensitive)\n",
        "        tokens = sql_query.split(\" \")\n",
        "        upper_tokens = [token.upper() for token in tokens]\n",
        "\n",
        "        try:\n",
        "            from_index = upper_tokens.index(\"FROM\")\n",
        "            tokens[from_index + 1] = \"dftbl\"\n",
        "        except ValueError:\n",
        "            raise ValueError(\"The SQL query does not contain a FROM clause.\")\n",
        "\n",
        "        new_query = \" \".join(tokens)\n",
        "\n",
        "        # Load CSV file into dataframe\n",
        "        dftbl = pd.DataFrame(csv_files_data[file_name])\n",
        "\n",
        "        # Run query using duckdb\n",
        "        result_df = duckdb.query(new_query).to_df()\n",
        "\n",
        "        return result_df.to_dict()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1740429018293
        }
      },
      "outputs": [],
      "source": [
        "# Setting up the agent's function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1acfb796-0418-4317-8bb1-3c30999d4f20",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "gather": {
          "logged": 1740429019339
        }
      },
      "outputs": [],
      "source": [
        "from mlflow.types.llm import ChatMessage, ChatParams, ChatChoice\n",
        "\n",
        "def my_agent_function(\n",
        "    messages: List[ChatMessage], params: Optional[ChatParams] = None, verbose: bool = False\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Executes a request using a chat agent with designated tools to generate responses.\n",
        "\n",
        "    Args:\n",
        "        messages (List[ChatMessage]): A list of ChatMessage objects representing the\n",
        "                                       conversation history.\n",
        "        params (Optional[ChatParams]): Optional parameters for the chat. Defaults to None.\n",
        "        verbose (bool): If True, enables verbose logging. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        ChatCompletionResponse: The generated response from the chat agent.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the messages list is empty.\n",
        "    \"\"\"\n",
        "    if not messages:\n",
        "        raise ValueError(\"The messages list cannot be empty.\")\n",
        "\n",
        "    ask = messages[-1].content  # MFlow Signature requires both inputs to be arrays\n",
        "\n",
        "    chat_history = [\n",
        "        {\"role\": message.role, \"content\": message.content} for message in messages[:-1]\n",
        "    ]\n",
        "\n",
        "    agent_tools = [\n",
        "        GetDatasetMetadaTool(),  # noqa\n",
        "        GetTargetFileSchemaTool(),  # noqa\n",
        "        GenerateSQLForFileTool(),  # noqa\n",
        "        GenerateAnswerforQuestionTool(),  # noqa\n",
        "    ]\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"\"\"You are a helpful agent who receives questions and answers them\n",
        "                    with the tools available. If none of your tools are able to provide\n",
        "                    the necessary answer, respond saying you are not able to assist with\n",
        "                    that question based on the tools available to you.\"\"\",\n",
        "            ),\n",
        "            MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "            (\"human\", \"{input}\"),\n",
        "            MessagesPlaceholder(\"agent_scratchpad\"),\n",
        "        ]\n",
        "    )\n",
        "    #ws.environ[\"KONG_API_GATEWAY_ENDPOINT\"] = keyvault.get_secret(name=\"flow-ai-id\")\n",
        "    model = AzureChatOpenAI(\n",
        "        model=\"o1\",\n",
        "        azure_endpoint=\"https://dev.api.gsk.com/co/psc/azureopenai/us6/\", # os.getenv(\"o1_AZURE_OPENAI_ENDPOINT\"),\n",
        "        azure_deployment=\"o1\", # os.getenv(\"o1_AZURE_OPENAI_DEPLOYMENT\"),\n",
        "        api_version= \"2024-12-01-preview\", # os.getenv(\"o1_AZURE_OPENAI_API_VERSION\"),\n",
        "        azure_ad_token_provider=token_provider\n",
        "        )\n",
        "    \n",
        "\n",
        "    agent = create_openai_tools_agent(model, agent_tools, prompt)\n",
        "\n",
        "    agent_executor = AgentExecutor(\n",
        "        agent=agent,\n",
        "        tools=agent_tools,\n",
        "        verbose=verbose,\n",
        "        return_intermediate_steps=True,\n",
        "        handle_parsing_errors=True,\n",
        "    )\n",
        "\n",
        "    if isinstance(agent_executor.agent, RunnableMultiActionAgent) or isinstance(\n",
        "        agent_executor.agent, RunnableAgent\n",
        "    ):\n",
        "        agent_executor.agent.stream_runnable = False\n",
        "\n",
        "    res = agent_executor.invoke({\"input\": ask, \"chat_history\": chat_history})[\"output\"]\n",
        "\n",
        "    return {\n",
        "        \"choices\": [\n",
        "            {\n",
        "                \"message\": {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": res\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomTokenProvider:\n",
        "    def __init__(self, access_token):\n",
        "        self.access_token = access_token\n",
        " \n",
        "    def __call__(self, *scopes):\n",
        "        return self.access_token\n",
        " \n",
        " \n",
        "def create_bearer_token():\n",
        " \n",
        "    federation_url = \"https://federation-qa.gsk.com/as/token.oauth2\" # os.getenv(\"federation_url_token\")\n",
        "    client_id = os.environ.get(\"azure_openai_client\") # os.getenv(\"AZURE_CLIENT_ID\")\n",
        "    client_secret = os.environ.get(\"azure_openai_secret\")\n",
        "    grant_type = \"client_credentials\" # os.getenv(\"grant_type_token\")\n",
        "    client_id = keyvault.get_secret(name=\"flow-ai-id\")\n",
        "    client_secret= keyvault.get_secret(name=\"flow-ai-access\")\n",
        "    # Data to send in the request\n",
        "    token_data = {\n",
        "        \"grant_type\": grant_type,\n",
        "        \"client_id\": client_id,\n",
        "        \"client_secret\": client_secret\n",
        "    }\n",
        " \n",
        "    # Request a bearer token\n",
        "    token_response = requests.post(federation_url, data=token_data)\n",
        " \n",
        "    # Extract the access token from the response\n",
        "    if token_response.status_code == 200:\n",
        "        token_info = token_response.json()\n",
        "        access_token = token_info[\"access_token\"]\n",
        "        #print(f\"Access Token: {access_token}\")\n",
        "        return access_token\n",
        "    else:\n",
        "        #print(f\"Failed to get token: {token_response.text}\")\n",
        "        return \"Requst Failed\"\n",
        " \n",
        "# Your existing access token\n",
        "access_token = create_bearer_token()\n",
        " \n",
        "# Create the custom token provider\n",
        "token_provider = CustomTokenProvider(access_token)\n",
        " \n",
        "def model_client():\n",
        " \n",
        "    gpt4o_model_client = AzureChatOpenAI(\n",
        "    model=\"o1\",\n",
        "    azure_endpoint=\"https://dev.api.gsk.com/co/psc/azureopenai/us6/\", # os.getenv(\"o1_AZURE_OPENAI_ENDPOINT\"),\n",
        "    azure_deployment=\"o1\", # os.getenv(\"o1_AZURE_OPENAI_DEPLOYMENT\"),\n",
        "    api_version= \"2024-12-01-preview\", # os.getenv(\"o1_AZURE_OPENAI_API_VERSION\"),\n",
        "    model_capabilities={\"function_calling\": True, \"vision\": True, \"json_output\": True},\n",
        "    azure_ad_token_provider=token_provider\n",
        "    )\n",
        " \n",
        "    return gpt4o_model_client\n",
        "    \n",
        "def create_openai_client():\n",
        "    # Create OpenAI clientCha\n",
        "    client = AzureOpenAI(\n",
        "        api_version= \"2024-12-01-preview\", # os.getenv(\"o1_AZURE_OPENAI_API_VERSION\"),\n",
        "        azure_endpoint=\"https://dev.api.gsk.com/co/psc/azureopenai/us6/\", # os.getenv(\"o1_AZURE_OPENAI_ENDPOINT\"),\n",
        "        azure_ad_token= create_bearer_token(),\n",
        "        max_retries=4\n",
        "    )\n",
        "    return client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "7468406e-45f6-4897-8916-304b4f52eb0a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "gather": {
          "logged": 1740429052274
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_file_metadata` with `{'question': 'How many asian patients in the dataset?'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m{\"demographics\": {\"description\": \"Includes case-related demographic details such as ethnicity, gender, race, year of birth, and year of death. This data is essential for analyzing trends and patterns across different demographic groups, assisting in decision-making and resource allocation. The table helps understand the diversity and characteristics of the studied population, providing insights into disparities or specific demographic needs.\", \"sample_schema\": [{\"case_id\": \"0030a28c-81aa-44b0-8be0-b35e1dcbf98c\", \"ethnicity\": \"not hispanic or latino\", \"gender\": \"female\", \"race\": \"asian\", \"year_of_birth\": 1952.0, \"year_of_death\": 2002.0, \"file_id\": \"0bd755e8-752f-49f0-9574-ea7813a7f0e0\"}]}, \"diagnoses\": {\"description\": \"Includes data on tumor diagnoses, such as tumor classification, primary diagnosis, tissue or organ of origin, treatments administered, tumor grade, and file ID. It tracks and manages tumor diagnosis cases, offering insights into tumor types, origins, and treatments. This data is vital for healthcare professionals and researchers to analyze trends in tumor diagnoses and treatment outcomes.\", \"sample_schema\": [{\"case_id\": \"0030a28c-81aa-44b0-8be0-b35e1dcbf98c\", \"classification_of_tumor\": \"not reported\", \"diagnosis_id\": \"a4ecd69c-6156-5653-973f-5d3446d68a40\", \"primary_diagnosis\": \"Malignant lymphoma, large B-cell, diffuse, NOS\", \"tissue_or_organ_of_origin\": \"Intra-abdominal lymph nodes\", \"treatments0_therapeutic_agents\": \"N/A\", \"treatments0_treatment_id\": \"9c611d7e-3ccf-5886-8743-53ab33fef092\", \"treatments1_therapeutic_agents\": \"N/A\", \"treatments1_treatment_id\": \"dcb0c9cf-1372-5f90-b9f6-3fbfcb8b3cb7\", \"treatments1_updated_datetime\": \"2019-07-31T21:32:22.377Z\", \"tumor_grade\": \"not reported\", \"file_id\": \"0bd755e8-752f-49f0-9574-ea7813a7f0e0\"}]}, \"exposures\": {\"description\": \"The 'exposures' table includes data on various exposures like alcohol history, alcohol intensity, cigarettes per day, and years smoked for different case IDs. This information is essential for analyzing the potential impact of these exposures on specific cases. The table serves as a repository for exposure data used in research, risk assessment, and personalized interventions.\", \"sample_schema\": [{\"case_id\": \"0030a28c-81aa-44b0-8be0-b35e1dcbf98c\", \"alcohol_history\": \"Not Reported\", \"alcohol_intensity\": \"N/A\", \"cigarettes_per_day\": 0, \"years_smoked\": 0, \"file_id\": \"0bd755e8-752f-49f0-9574-ea7813a7f0e0\"}]}}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_target_file_schema` with `{'question': 'How many asian patients in the dataset?', 'files_metadata': '{\"demographics\":{\"description\":\"Includes case-related demographic details such as ethnicity, gender, race, year of birth, and year of death. This data is essential for analyzing trends and patterns across different demographic groups, assisting in decision-making and resource allocation. The table helps understand the diversity and characteristics of the studied population, providing insights into disparities or specific demographic needs.\",\"sample_schema\":[{\"case_id\":\"0030a28c-81aa-44b0-8be0-b35e1dcbf98c\",\"ethnicity\":\"not hispanic or latino\",\"gender\":\"female\",\"race\":\"asian\",\"year_of_birth\":1952.0,\"year_of_death\":2002.0,\"file_id\":\"0bd755e8-752f-49f0-9574-ea7813a7f0e0\"}]},\"diagnoses\":{\"description\":\"Includes data on tumor diagnoses, such as tumor classification, primary diagnosis, tissue or organ of origin, treatments administered, tumor grade, and file ID. It tracks and manages tumor diagnosis cases, offering insights into tumor types, origins, and treatments. This data is vital for healthcare professionals and researchers to analyze trends in tumor diagnoses and treatment outcomes.\",\"sample_schema\":[{\"case_id\":\"0030a28c-81aa-44b0-8be0-b35e1dcbf98c\",\"classification_of_tumor\":\"not reported\",\"diagnosis_id\":\"a4ecd69c-6156-5653-973f-5d3446d68a40\",\"primary_diagnosis\":\"Malignant lymphoma, large B-cell, diffuse, NOS\",\"tissue_or_organ_of_origin\":\"Intra-abdominal lymph nodes\",\"treatments0_therapeutic_agents\":\"N/A\",\"treatments0_treatment_id\":\"9c611d7e-3ccf-5886-8743-53ab33fef092\",\"treatments1_therapeutic_agents\":\"N/A\",\"treatments1_treatment_id\":\"dcb0c9cf-1372-5f90-b9f6-3fbfcb8b3cb7\",\"treatments1_updated_datetime\":\"2019-07-31T21:32:22.377Z\",\"tumor_grade\":\"not reported\",\"file_id\":\"0bd755e8-752f-49f0-9574-ea7813a7f0e0\"}]},\"exposures\":{\"description\":\"The \\'exposures\\' table includes data on various exposures like alcohol history, alcohol intensity, cigarettes per day, and years smoked for different case IDs. This information is essential for analyzing the potential impact of these exposures on specific cases. The table serves as a repository for exposure data used in research, risk assessment, and personalized interventions.\",\"sample_schema\":[{\"case_id\":\"0030a28c-81aa-44b0-8be0-b35e1dcbf98c\",\"alcohol_history\":\"Not Reported\",\"alcohol_intensity\":\"N/A\",\"cigarettes_per_day\":0,\"years_smoked\":0,\"file_id\":\"0bd755e8-752f-49f0-9574-ea7813a7f0e0\"}]}}'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m{'file_name': 'demographics', 'file_schema': 'case_id, ethnicity, gender, race, year_of_birth, year_of_death, file_id'}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `generate_sql_for_file` with `{'question': 'How many asian patients in the dataset?', 'file_name': 'demographics', 'file_schema': 'case_id, ethnicity, gender, race, year_of_birth, year_of_death, file_id'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mSELECT COUNT(*) AS total_asian_patients FROM demographics WHERE lower(race) = 'asian';\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `generate_answer_for_question` with `{'question': 'How many asian patients in the dataset?', 'sql_query': \"SELECT COUNT(*) AS total_asian_patients FROM demographics WHERE lower(race) = 'asian';\", 'file_name': 'demographics', 'file_schema': 'case_id, ethnicity, gender, race, year_of_birth, year_of_death, file_id'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m{'total_asian_patients': {0: 627}}\u001b[0m\u001b[32;1m\u001b[1;3mThere are 627 Asian patients in the dataset.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'choices': [{'message': {'role': 'assistant',\n",
              "    'content': 'There are 627 Asian patients in the dataset.'}}]}"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# To test the agent uncomment the below\n",
        "#import os\n",
        "# os.environ[\"KONG_CLIENT_ID\"] = keyvault.get_secret(name=\"flow-ai-id\")  # <-- Set your actual client ID here\n",
        "# os.environ[\"KONG_CLIENT_SECRET\"] = keyvault.get_secret(name=\"flow-ai-access\")  # <-- Set your actual client secret here\n",
        "\n",
        "input_messages = [\n",
        "    ChatMessage(role=\"user\", content=\"How many asian patients in the dataset?\"),\n",
        "]\n",
        "#params_with_custom_inputs = ChatParams(custom_inputs={\"client_type\": \"mobile\"})\n",
        "params_with_custom_inputs = ChatParams()\n",
        "\n",
        "my_agent_function(messages=input_messages, params=params_with_custom_inputs, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "241eeb72-0063-48f1-9de7-9fe98f176e66",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Another test\n",
        "\n",
        "# input_messages = [\n",
        "#     ChatMessage(role=\"user\", content=\"Describe the underlying datasources of the dataset.\"),\n",
        "# ]\n",
        "# params_with_custom_inputs = ChatParams(custom_inputs={\"client_type\": \"mobile\"})\n",
        "\n",
        "# print(my_agent_function(messages=input_messages, params=params_with_custom_inputs).choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "agent_langchain",
      "widgets": {}
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
